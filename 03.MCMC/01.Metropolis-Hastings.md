We want to sample from a very simple discrete probability distribution over two states, let's call them State A and State B. We don't know the exact probabilities, but we know they are *proportional* to some values. Let the target distribution (unnormalized) be: </br></br>
$\pi(A) = 3$
$\pi(B) = 1$

This means the true probabilities are $P(A) = \frac{3}{3+1} = 0.75$ and $P(B) = \frac{1}{3+1} = 0.25$. Our MCMC algorithm should eventually spend about 75% of its time in State A and 25% in State B.

**Components:**

1.  **Target Distribution (unnormalized):** $\pi(x)$, where $x \in \{A, B\}$. We have $\pi(A)=3$ and $\pi(B)=1$.
2.  **Proposal Distribution:** $Q(x' | x)$. This proposes a *new* state $x'$ given the *current* state $x$. Let's use a super simple proposal: always propose switching to the *other* state.
    *   If current state is A, propose B: $Q(B | A) = 1$
    *   If current state is B, propose A: $Q(A | B) = 1$
    *   (Implicitly, $Q(A | A) = 0$ and $Q(B | B) = 0$).
    Notice that this proposal distribution is symmetric: $Q(x' | x) = Q(x | x') = 1$ if $x \neq x'$.
3.  **Acceptance Probability:** $\alpha(x' | x)$. This is the probability of accepting the proposed state $x'$. The general formula is:
    $\alpha(x' | x) = \min \left( 1, \frac{\pi(x') Q(x | x')}{\pi(x) Q(x' | x)} \right)$
    Since our $Q$ is symmetric ($Q(x|x') = Q(x'|x)$), this simplifies to the Metropolis acceptance probability:
    $\alpha(x' | x) = \min \left( 1, \frac{\pi(x')}{\pi(x)} \right)$

**The Algorithm Steps:**

1.  **Initialization:** Start at an arbitrary state. Let's start at $x_0 = A$.
2.  **Iteration (t=0, 1, 2, ...):**
    a.  You are currently at state $x_t$.
    b.  **Propose** a new state $x'$ according to $Q(x' | x_t)$.
        *   If $x_t = A$, propose $x' = B$.
        *   If $x_t = B$, propose $x' = A$.
    c.  **Calculate** the acceptance probability $\alpha(x' | x_t) = \min \left( 1, \frac{\pi(x')}{\pi(x_t)} \right)$.
    d.  **Decide** to accept or reject:
        *   Generate a random number $u$ from a Uniform(0, 1) distribution.
        *   If $u < \alpha(x' | x_t)$, accept the proposal: set $x_{t+1} = x'$.
        *   Otherwise (if $u \ge \alpha(x' | x_t)$), reject the proposal: set $x_{t+1} = x_t$ (stay in the current state).
3.  Repeat step 2 many times. The sequence of states $x_0, x_1, x_2, ...$ is your Markov chain.

**Example Walkthrough:**

*   **t=0:** Current state $x_0 = A$.
    *   Propose $x' = B$.
    *   Calculate $\alpha(B | A) = \min(1, \pi(B)/\pi(A)) = \min(1, 1/3) = 1/3$.
    *   Generate $u$. Let's say $u = 0.6$.
    *   Is $u < \alpha$? Is $0.6 < 1/3$? No.
    *   Reject. $x_1 = x_0 = A$. Chain: [A]
*   **t=1:** Current state $x_1 = A$.
    *   Propose $x' = B$.
    *   Calculate $\alpha(B | A) = 1/3$.
    *   Generate $u$. Let's say $u = 0.2$.
    *   Is $u < \alpha$? Is $0.2 < 1/3$? Yes.
    *   Accept. $x_2 = x' = B$. Chain: [A, B]
*   **t=2:** Current state $x_2 = B$.
    *   Propose $x' = A$.
    *   Calculate $\alpha(A | B) = \min(1, \pi(A)/\pi(B)) = \min(1, 3/1) = \min(1, 3) = 1$.
    *   Generate $u$. Let's say $u = 0.85$.
    *   Is $u < \alpha$? Is $0.85 < 1$? Yes. (Note: we *always* accept when moving from B to A because $\alpha=1$).
    *   Accept. $x_3 = x' = A$. Chain: [A, B, A]
*   **t=3:** Current state $x_3 = A$.
    *   Propose $x' = B$.
    *   Calculate $\alpha(B | A) = 1/3$.
    *   Generate $u$. Let's say $u = 0.9$.
    *   Is $u < \alpha$? Is $0.9 < 1/3$? No.
    *   Reject. $x_4 = x_3 = A$. Chain: [A, B, A, A]

...and so on.

**Outcome:** If you run this for many steps, the list of states (samples) generated will contain approximately 75% 'A's and 25% 'B's, thus successfully sampling from the target distribution $P(x)$. (Usually, you discard the first part of the chain, called "burn-in", to let it reach the stationary distribution).