# Metropolis-Hastings (M-H) MCMC

## **1. Introduction**

*   **Markov Chain Monte Carlo (MCMC):** A class of algorithms used to draw samples from a probability distribution, especially when direct sampling is difficult or impossible. This often happens when the distribution is high-dimensional or we only know it up to a normalizing constant.
*   **Metropolis-Hastings (M-H):** A specific and widely used MCMC algorithm. It constructs a Markov chain whose stationary distribution is the desired target distribution.
*   **Example:** To illustrate the M-H mechanism in the simplest possible setting, let's take sampling from a discrete distribution over just two states where we know the *relative* probabilities.

## **2. Problem Setup: The Target Distribution**

We want to generate samples (A or B) according to a target probability distribution $P(x)$. We don't know $P(x)$ directly, but we know a function $\pi(x)$ that is *proportional* to it, meaning $P(x) \propto \pi(x)$. This $\pi(x)$ is often called the **unnormalized target distribution**.

*   **States:** Our system can be in one of two states: $x \in \{A, B\}$.
*   **Unnormalized Probabilities:**
    *   $\pi(A) = 3$
    *   $\pi(B) = 1$
*   **Interpretation:** State A is 3 times more likely than State B.
*   **(For Verification):** The true, normalized probabilities are $P(A) = \frac{\pi(A)}{\pi(A) + \pi(B)} = \frac{3}{3+1} = 0.75$ and $P(B) = \frac{\pi(B)}{\pi(A) + \pi(B)} = \frac{1}{3+1} = 0.25$. Our MCMC samples should eventually reflect this 3:1 ratio. M-H works *without* needing to calculate this normalization constant (3+1=4).

## **3. The Metropolis-Hastings Components**

The M-H algorithm needs two key components besides the target distribution:

*   **a) Target Distribution (unnormalized):** $\pi(x)$, where $x \in \{A, B\}$. We have $\pi(A)=3$ and $\pi(B)=1$.

*   **b) Proposal Distribution $Q(x' | x)$:**
    *   **Purpose:** Given the current state $x$, this distribution suggests a *candidate* next state $x'$. It defines how we explore the state space.
    *   **Our Choice:** A deterministic proposal: always propose switching to the *other* state.
        *   If current state is A, propose B: $Q(B | A) = 1$
        *   If current state is B, propose A: $Q(A | B) = 1$
        *   (Implicitly, $Q(A | A) = 0$ and $Q(B | B) = 0$)
    *   **Symmetry:** Note that $Q(B | A) = Q(A | B) = 1$. When $Q(x' | x) = Q(x | x')$, the proposal distribution is called **symmetric**. This simplifies the acceptance probability calculation: $Q(x' | x) = Q(x | x') = 1$ if $x \neq x'$

*   **c) Acceptance Probability $\alpha(x' | x)$:**
    *   **Purpose:** After proposing state $x'$, we need to decide whether to *accept* this move (jump to $x'$) or *reject* it (stay at $x$). This probability ensures that, in the long run, we spend the correct amount of time in each state according to $\pi(x)$.
    *   **General Formula:**
        $\alpha(x' | x) = \min \left( 1, \frac{\pi(x') Q(x | x')}{\pi(x) Q(x' | x)} \right)$
    *   **Simplified Formula (for symmetric Q):** Since $Q(x | x') = Q(x' | x)$ in our case, the Q terms cancel out, giving the simpler **Metropolis** acceptance probability:
        $\alpha(x' | x) = \min \left( 1, \frac{\pi(x')}{\pi(x)} \right)$
    *   **Intuition:** We always accept a move to a *more* probable state (ratio $\pi(x')/\pi(x) > 1$). We *might* accept a move to a *less* probable state (ratio < 1), with a probability equal to that ratio. This allows the chain to explore the whole space but biases it towards higher probability regions.
    *   **Calculating for our example:**
        *   Moving $A \to B$: $\alpha(B | A) = \min \left( 1, \frac{\pi(B)}{\pi(A)} \right) = \min \left( 1, \frac{1}{3} \right) = \frac{1}{3}$
        *   Moving $B \to A$: $\alpha(A | B) = \min \left( 1, \frac{\pi(A)}{\pi(B)} \right) = \min \left( 1, \frac{3}{1} \right) = 1$

## **4. The Algorithm Steps**

1.  **Initialization:** Start at an arbitrary state $x_0$. Let's start at $x_0 = A$.</br>
2.  **Iterate (for $t = 0, 1, 2, ..., N-1$):**</br>
    a.  Let the current state be $x_t$.</br>
    b.  **Propose:** Generate a candidate state $x'$ from the proposal distribution $Q(x' | x_t)$.
        *(In our case: if $x_t=A$, $x'=B$; if $x_t=B$, $x'=A$).*</br>
    c.  **Calculate:** Compute the acceptance probability $\alpha(x' | x_t)$.
        *(In our case: if proposing B from A, $\alpha=1/3$; if proposing A from B, $\alpha=1$)*: $\:$ $\alpha(x' | x_t) = \min \left( 1, \frac{\pi(x')}{\pi(x_t)} \right)$</br>
    d.  **Decide:** Generate a random number $u$ from a Uniform(0, 1) distribution.
        *   If $u < \alpha(x' | x_t)$: **Accept** the proposal. Set the next state $x_{t+1} = x'$.
        *   Else ($u \ge \alpha(x' | x_t)$): **Reject** the proposal. Set the next state $x_{t+1} = x_t$ (stay in the current state).</br>
3.  **Output:** Repeat step 2 many times. The sequence of states $x_0, x_1, x_2, ..., x_N$  is your Markov chain..

## **5. Example Walkthrough**

*   **t=0:** Current state $x_0 = A$.
    *   Propose $x' = B$.
    *   Calculate $\alpha(B | A) = \min(1, \pi(B)/\pi(A)) = \min(1, 1/3) = 1/3$.
    *   Generate $u$. Let's say $u = 0.6$.
    *   Is $u < \alpha$? Is $0.6 < 1/3$? No.
    *   Reject. $x_1 = x_0 = A$. Chain: [A]
*   **t=1:** Current state $x_1 = A$.
    *   Propose $x' = B$.
    *   Calculate $\alpha(B | A) = 1/3$.
    *   Generate $u$. Let's say $u = 0.2$.
    *   Is $u < \alpha$? Is $0.2 < 1/3$? Yes.
    *   Accept. $x_2 = x' = B$. Chain: [A, B]
*   **t=2:** Current state $x_2 = B$.
    *   Propose $x' = A$.
    *   Calculate $\alpha(A | B) = \min(1, \pi(A)/\pi(B)) = \min(1, 3/1) = \min(1, 3) = 1$.
    *   Generate $u$. Let's say $u = 0.85$.
    *   Is $u < \alpha$? Is $0.85 < 1$? Yes. (Note: we *always* accept when moving from B to A because $\alpha=1$).
    *   Accept. $x_3 = x' = A$. Chain: [A, B, A]
*   **t=3:** Current state $x_3 = A$.
    *   Propose $x' = B$.
    *   Calculate $\alpha(B | A) = 1/3$.
    *   Generate $u$. Let's say $u = 0.9$.
    *   Is $u < \alpha$? Is $0.9 < 1/3$? No.
    *   Reject. $x_4 = x_3 = A$. Chain: [A, B, A, A]

...and so on.

| Step (t) | Current State ($x_t$) | Proposed State ($x'$) | $\pi(x')$ | $\pi(x_t)$ | Ratio $\frac{\pi(x')}{\pi(x_t)}$ | Acceptance Prob. $\alpha = \min(1, \text{ratio})$ | Random $u \sim U(0,1)$ | $u < \alpha$? | Decision | Next State ($x_{t+1}$) | Chain |
| :------- | :-------------------- | :-------------------- | :-------- | :-------- | :------------------------------------ | :----------------------------------------- | :---------------------- | :---------- | :--------- | :-------------------- | :------ |
| 0        | A                     | B                     | 1         | 3         | 1/3                                   | 1/3                                        | 0.6                     | No          | Reject     | A                     | [A]     |
| 1        | A                     | B                     | 1         | 3         | 1/3                                   | 1/3                                        | 0.2                     | Yes         | Accept     | B                     | [A, B]  |
| 2        | B                     | A                     | 3         | 1         | 3                                     | 1                                          | 0.85                    | Yes         | Accept     | A                     | [A, B, A] |
| 3        | A                     | B                     | 1         | 3         | 1/3                                   | 1/3                                        | 0.9                     | No          | Reject     | A                     | [A, B, A, A] |
| 4        | A                     | B                     | 1         | 3         | 1/3                                   | 1/3                                        | 0.15                    | Yes         | Accept     | B                     | [A, B, A, A, B] |
| 5        | B                     | A                     | 3         | 1         | 3                                     | 1                                          | 0.5                     | Yes         | Accept     | A                     | [A, B, A, A, B, A] |

... and so on.

## **6. Interpretation and Outcome**

*   **The Output Chain:** The sequence $[A, B, A, A, B, A, ...]$ is our set of samples.
*   **Burn-in:** Typically, the initial part of the chain (e.g., the first few or many steps) is discarded. This is called the "burn-in" period, allowing the chain to forget its starting position and reach its stationary distribution.
*   **Sampling:** After burn-in, the remaining states in the chain are treated as (correlated) samples from the target distribution $P(x)$.
*   **Expected Result:** If we run the chain for a long time (many steps N), the proportion of times we observe state A will approach $P(A)=0.75$, and the proportion of times we observe state B will approach $P(B)=0.25$. The algorithm successfully samples the desired distribution without ever needing the normalization constant.
*   **Why it Works:** The construction of the acceptance probability $\alpha$ ensures a property called "detailed balance" with respect to $\pi(x)$. This guarantees that the stationary distribution of the Markov chain is indeed the target distribution $P(x) \propto \pi(x)$.

## **7. Key Takeaways**

**Outcome:** If you run this for many steps, the list of states (samples) generated will contain approximately 75% 'A's and 25% 'B's, thus successfully sampling from the target distribution $P(x)$. (Usually, you discard the first part of the chain, called "burn-in", to let it reach the stationary distribution).

*   M-H allows sampling from a distribution $\pi(x)$ even if we don't know its normalization constant.
*   It works by proposing moves ($Q$) and accepting/rejecting them based on the ratio of target probabilities ($\alpha$).
*   Moves to more probable states are preferred, but moves to less probable states are sometimes allowed, enabling exploration.
*   After a burn-in period, the sequence of states generated represents samples from the target distribution.

---